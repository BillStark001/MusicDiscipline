# NLP2

-by 刘泽禹 张心澄 余锴诚

![cover](C:\Users\tony\Desktop\RDFZ\NLP\NLP#2\cover.JPG)

那我们从这一讲开始进入一些正式内容啦~

#### 自然语言处理的组成部分

在上一讲中我们已经粗略地了解到了自然语言处理的作用，那么为了搭建一个完整的自然语言处理系统，我们都需要做哪些部分呢？

一个完整的自然语言处理系统应该至少包括这三个部分：**语言的解析**、**语义的理解**和**语言的生成**。

- **语言的解析**，主要包含对于词语词性的标注，和对文本中的语义块、语义角色的识别与标注。

- **语义的理解**，顾名思义主要是令机器对于给定的文本进行“理解”，并转换成机器可以识别的语言。

- **语言的生成**较好理解，指的就是机器生成一段完整的文本，并能够通顺地表达预期的含义。



#### 语言学

既然是处理语言，那么我们需要先了解一些语言学的知识。

语言是一个很抽象也很大的概念，显然不利于我们直接入手。幸运的是，语言并不是一个铁板一块完整的东西，而是可以进行拆解的。

回想一下你是怎么整理自己的语言的。【手动滑稽】

![Meme](C:\Users\tony\Desktop\RDFZ\NLP\NLP#2\Meme.JPG) 

很明显，假如你想要特定的含义，那么首当其冲的，你会先去思考你想要用的词语，所谓“炼字”很多时候就是这一过程的一种极致。然后，在得到了几个关键词之后，你会快速地把他们组合在一起，形成句子。最后再将这些句子排列组合一下成为段落、篇章。

这个小例子里面告诉了我们什么呢？

![Language Structure](C:\Users\tony\Desktop\RDFZ\NLP\NLP#2\Language Structure.JPG)

正如上图中我们可以看到的，与组织语言的过程相类似，语言虽然结构复杂，但其中仍然还会有基本的构成单位。而一段再复杂的文本，也是由这一些基本单位排列组合构建而成的。因此，在处理大段文本的时候，将文本还原成基本单位（即单个词语）是一种有价值的思路与方法。

#### 信息学

这一讲里最重要的部分来啦！（敲黑板！）

我们在这里主要学习分词技术。所谓“分词”，就是在一大段文本中，将基本的词语识别并割裂开来。这虽然看起来十分trivial和简单，但是实际上并不是这样的。考虑以下的一段文本：

```c++
国务院总理李克强调研上海外高桥
```

作为一个~~不~~正常的人类，笔者一眼扫过去就不难知道，这段文本中的基本词语包括“国务院总理”、“李克强”、“调研”和“上海外高桥”几个部分。但对于机器来说，要怎么知道要在“强”和“调”之间断开一下，而不是把“强调”分成一个词呢？想一想是不是还是有点困难呢？

分词技术是自然语言处理系统必不可少的成分，只有实现了分词、词性标注等语法解析，机器才能够进行接下来的语意解析和语言生成。相当于“听懂”了人类的语言，并作出自己的回复。可以说分词技术是NLP中最为基础和基本的内容。

那么分词需要实现那些东西呢？基本的分词技术需要实现最基本的功能包括tokenization和segmentation。

~~笔者在这里先来客串一下英语老师【大雾】~~。我们来看一下这两个词的定义：

```
tokenization: Break (text) into individual linguistic units.
															-Oxford English Dictionary
segmentation: Division into separate parts or sections.
															-Oxford English Dictionary
```

所谓的tokenization，就是将一段文字分割为一个个单独的语言单位；而segmentation，则是将文字中的不同语段分割开。

这...这有什么区别啊【汗

事实上，两者之间的区别还是很大的。

像英语这样的拼音语言中，最小有意义语言单位很容易识别（因为词与词之间存在空格），因此切分和标记相对容易。但像汉语这类象形文字中问题就变得复杂起来了。下面这张图就是一个很好的例子。

![Segmentation](C:\Users\tony\Desktop\RDFZ\NLP\NLP#2\Segmentation.JPG)

<center>(Mariana Neves, Universität Potsdam, 2017)</center>

因此tokenization主要针对的是较简单、基础的拼音语言，而segmentation则对更复杂的象形语言进行对有区分标记的最小语义单位，即句子，内部的语境内最小有意义语言单位，即字或者词，进行的切分。

作为中国人，我们自然更关注针对汉语的分词技术。如上所述，汉语分词歧义性最主要的来源是相较于拼音语言其最小语义单位间无明显区分标志所导致的机器顺序切分时产生的多种选择。当然，汉语中众多传递语境的虚词也为这项工作增添了许多麻烦。

现在的汉语分词法可以分为三大类：基于词典、基于规则或者基于统计。我们来分别看一看：

- 所谓基于词典，顾名思义就是讲一段文本与字典中的常用词语、用法进行比对。通过比较“教条”的匹配方法进行分词。但是由于词典词条数有限，无法灵活应对。同时回想上一个“国务院总理李克强调研”的例子，如果基于字符串匹配的规则，这一串文本可能就会有多种匹配方法，如何取舍就又是一个困难的问题。
- 所谓基于规则，也非常好理解。语言中总是有规则可寻的，遣词造句也有一定定式的规则。那么我们只需要发觉出这种规则，那么这些规则就可以被利用做分词的规则，得到符合规则的结果。然鹅语言的过于复杂则注定这种方法也不会容易。

- 所谓基于统计，就是基于词频度统计的分词方法。不同的词语有着不同的使用频率，因此如果一段文本可以有多种方式分段，使用频率高的分段方法更可能更贴近语意。来看两种基于统计的应用实例：

  “N-最短路径方法”是一种基于图论最短路算法的模型。该算法先对每个词语按照使用频率设立估值。不难发现，每一个切分出来的词语都会有左右两个端点，“N-最短路径法”在左右两端点之间建边，边权就是被左右两端点包含的词语的估值。然后该算法从起点开始计算以终点为最长路。那么按照贪心的思想，总估值最高的方案应该是最优的分词方式。

  ![N-Shortest Path](C:\Users\tony\Desktop\RDFZ\NLP\NLP#2\N-Shortest Path.JPG)

  “n元语法模型”同样是一种基于图论的模型。不同的是，在建边时，该算法考虑到了词语搭配的情况。在日常生活总，虽然很多词语都非常常见，使用频度很高，但组合起来却不那么常见。例如，“睡觉”和“大笑”可能都是很常用的词语，可很少有人会一边睡觉一边大笑不是吗？因此在建图的时候，假设切割端点前后的两个词语分别是 $u$  和 $v$ ，那么连接这两个点的边权则是“当前一个次为 $u$ ，后一个词为 $v$ 的概率”，用条件概率的形式表示即为： $P(v|u)$ 。

  ![N-Grammar](C:\Users\tony\Desktop\RDFZ\NLP\NLP#2\N-Grammar.JPG)

  令一个切分方法为 $S$， 切分结果为 $W$，其中每个得到的独立的词语分别是 $W_1， W_2, \dots,  W_n$ , 那么根据条件概率的贝叶斯公式，切分结果 $W$ 发生的概率，即切分方式 $S$ 的估值就应为：
  $$
  P(W) = P(W_1) \times P(W_2 |W_1) \times P(W_3|W_2)\times \dots \times P(W_n|W_{n - 1})
  $$
  通过这种方法计算得到最长链长度即为最优分词方式。

在现实中，很多的分词也不需要自己手写实现！**nltk(Natural Language Toolkit)** 和 **jieba** 是两个在python常用的分词库。只需要引用就可以使用封装好的分词命令啦！这里就不赘述这两个库的用法，只在这里给出reference的地址，读者可以自行探寻两个库的用法。

- nltk: http://www.nltk.org/
- jieba: https://github.com/fxsjy/jieba

这一期的学习笔记就到这里吧~ 

